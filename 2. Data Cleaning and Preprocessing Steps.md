![image](https://github.com/user-attachments/assets/ac7eb556-9d83-4b2e-8c09-b9a0dfd6869e)

## What is data cleaning? 
Data cleaning, also known as data cleansing or data scrubbing, is the process of identifying and correcting (or removing) errors, inconsistencies, and inaccuracies within a dataset.

The importance of data cleaning lies in the following factors:

- Improved data quality: It is therefore very important to clean the data as this reduces the chances of errors, inconsistencies and missing values, which ultimately makes the data to be more accurate and reliable in the analysis.
- Better decision-making: Consistent and clean data gives organization insight into comprehensive and actual information and minimizes the way such organizations make decisions on outdated and incomplete data.
- Increased efficiency: High quality data is efficient to analyze, model or report on it, whereas clean data often avoids a lot of foreseen time and effort that goes into handling poor data quality.


| Common Data Quality Issues | Example                |
|----------------------------| -------------------------- |
| Missing Values             | Absence of a value in a dataset. |
| Duplicate Data | Identical or nearly identical records appearing multiple times in a dataset. |
| Incorrect Data Type | String data represented as Int data type | 
| Formatting Inconsistencies | Creating a standardized date format and capitalization.|
|Outliers and Anomalies | Observations whose values are unusually high or low compared to other observations|
| Spelling / Typographical Errors | Misspelling and typos can occur in qualitative data |


## Common Python libraries used for general-purpose data cleaning

### Pandas

  Key Features for Data Cleaning:
- Handling Missing Data: `isnull()`, `dropna()`, `fillna()`.
- Removing Duplicates: `duplicated()`, `drop_duplicates()`.
- Data Type Conversion: `astype()`.
- Filtering and Subsetting Data: Boolean indexing, `query()`.
- String Operations: `str.contains()`, `str.replace()` for cleaning text data.

### NumPy - A fundamental package for numerical computations, often used with pandas for more advanced operations.

  Key Features for Data Cleaning:
- Handling Missing Data: NumPy arrays represent missing values as `np.nan`.
- Data Transformation: Apply mathematical or logical operations efficiently.
- Filtering Data: Boolean masks.

 How to install NumPy? 

In terminal, run the following command:
```Powershell
  pip install numpy
```

Other things to install - Visualizations
```Powershell
  pip install matplotlib
  pip install seaborn
```

## Implementing data cleaning funtions

#### Unique Value Count

```Python
df.YOUR-DESIRED-COLUMN.nunique()
```

#### Checking for Duplicates

```Python
# Identify duplicate rows
duplicates = df.duplicated()
print(duplicates)
```

#### Dropping Duplicates 

```Python
df.drop_duplicates(inplace=True)
```

#### Missing Data? -> Finding and printing null values 

```Python
rows_with_missing = df.loc[df.isnull().any(axis=1)]
print(rows_with_missing)
```

#### Plot Missing Values
##### Why is plotting Missing Values important? Please refer to the 'Statistics for Filling Missing Data' page
```Python
import seaborn as sns
sns.heatmap(df.isna(),cmap = 'Greens')
```

#### Removing Missing Data

```Python
# Drop rows with any missing values in place
df.dropna(inplace=True)
```

#### Filling Missing Data
##### When dealing with missing data, it's often necessary to use statistical methods to determine the best way to fill in those gaps. Please refer to the 'Statistics for Filling Missing Data' page to find out more. 
```Python
# Replace missing values with anything, in this case it will be 0's
df = df.fillna(0)
print(df)
```

#### Locating Zero's 

```Python
df.loc[(df==0).any(axis=1)]
```

#### Removing Zero's

```Python
# Locate rows with any zero values
rows_with_zeros = df.loc[(df == 0).any(axis=1)]

# Remove those rows from the original DataFrame
df = df.drop(rows_with_zeros.index)
```

#### .astype() - Changing a columns data type (ie. from string to int)

```Python
#We are changing the datatype of 'column6name' to be an integer data type. 
df['column6name'] = df['column6name'].astype(int)
#Referencce Data Loading and Inspection to find out how to get the data types of your dataset. 
```

#### Stripping - Eliminating a specific characteristic from the front/back end of a word or name. 

```python
#Have to combine everything you want to strip into a single string

#left strip
df['column6name] = df['column6name'].str.lstrip("/._")
#Right Strip
df['column6name] = df['column6name'].str.rstrip("/._")
#Regular strip (applicable on both sides)
df['column6name] = df['column6name'].str.strip("/._")
```

#### Checking for substrings using str.contains()

```Python
df = df[df['City'].str.contains('New')]  # Filter rows where 'City' contains 'New'
print(df)
```

#### Replacing using str.replace()

```Python
df['City'] = df['City'].str.replace('New', 'Old')  # Replace 'New' with 'Old' in 'City' column
print
```

## Filtering & Querying

#### Boolean Indexing

```Python
filtered_df = df[df['Age'] > 30]  # Filter rows where 'Age' is greater than 30
print(filtered_df)
```

#### Query Methos

```Python
filtered_df = df.query('Age > 30')  # Filter rows where 'Age' is greater than 30
print(filtered_df)
```

## Finding Outliers

#### Detecting Outliers using describe function from page 1. 

```Python
df['YOUR-DESIRED-COLUMN'].describe()
```

#### Detecting Outliers with BoxPlot visualization

```Python
import seaborn as sns
sns.boxplot(data=df[['YOUR-DESIRED-COLUMN1', 'YOUR-DESIRED-COLUMN2']])
```

#### Remove outliers with quantiles 

##### Quantile-based methods are widely used in statistics to handle outliers and to make data more manageable and representative for analysis. By removing values below the 1st percentile and above the 99th percentile, you essentially filter out the most extreme outliers, which can help in creating a more robust and accurate analysis.

```Python
q_low = df['YOUR-DESIRED-COLUMN1'].quantile(0.01) #Calculate the lower quantile (1st percentile)
q_hi  = df['YOUR-DESIRED-COLUMN1'].quantile(0.99) #Calculate the upper quantile (99th percentile)
df[(df['YOUR-DESIRED-COLUMN1'] < q_hi)&(df['YOUR-DESIRED-COLUMN1'] > q_low)] #Filter the DataFrame to remove outliers
```

#### Remove outliers with std

##### Standard Deviation: A measure of the dispersion or spread of a set of values. It quantifies the amount of variation or deviation from the mean. Empirical Rule: In a normal distribution:

-Approximately 68% of the data falls within one standard deviation of the mean.

-Approximately 95% of the data falls within two standard deviations of the mean.

-Approximately 99.7% of the data falls within three standard deviations of the mean.

By using three times the standard deviation as the threshold, you are essentially capturing around 99.7% of the data and considering the rest as outliers. This approach works well if your data is normally distributed.

``` Python
import numpy as np
ab = np.abs(df['YOUR-DESIRED-COLUMN1']-df['YOUR-DESIRED-COLUMN1'].mean()) #calculates the absolute difference between each value in the column and the mean. This gives us a measure of how far each value is from the mean.
std = (3*df['YOUR-DESIRED-COLUMN1'].std()) #This line calculates three times the standard deviation of the column
df[ab <= std ] #Filtering out outliers
```
https://datascientyst.com/pandas-cheat-sheet-data-cleaning/
